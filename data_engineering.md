## ETL

>ETL이란 기업이 전 세계 모든 곳의 수많은 팀에서 관리하는 구조화된 데이터와 구조화되지 않은 데이터를 비롯한 전체 데이터를 가져와 비즈니스 목적에 실질적으로 유용한 상태로 변환하는 엔드 투 엔드 프로세스를 의미합니다.

>ETL은 추출, 변환, 적재(Extract, Transform, Load)의 약자로 동일 기종 혹은 이기종의 원천데이터로부터 데이터 웨어하우스에서 쌓는 과정을 뜻하는데 변환(Transform) 과정이 무척 많은 노력이 투여된다. 반면에 ELT는 데이터를 먼저 적재한 후에 필요에 따라 변환과정을 거쳐 후속 작업에 사용한다. 데이터 호수 ELT 프로세스가 매력적으로 보이지만 데이터 카탈로그가 잘 관리되지 않는다면 데이터 늪(Data Swamp)가 될 수 있다.

#### extract
- staging 영역에 저장 

#### transform
- 정리 단계
- 데이터 클렌징 작업
- load 할 타겟에 맞게 데이터를 정리 하는 단계 ( 스키마 정의를 하는 부분도 포함 ) 
- 중복제거, null처리, 오류 데이터 제거 

#### load
- 형식이 지정된 데이터를 실제 DB, data lake, DW 등에 삽입하는 작업. 


## ETL 작업 프로세스
 - Step 0(interface)
    • 다양한 데이터 원천(Source)으로부터 데이터를 획득하기 위한 인터페이스 매커니즘 구현
 - Step 1(Staging ETL)
    • 획득된 데이터를 스테이징 테이블에 저장
 - Step 2(Profiling ETL)
    • 스테이징 테이블에서 데이터 특성을 식별하고 품질을 측정
 - Step 3(Cleansing ETL)
    • 다양한 규칙들을 활용해 프로파일링된 데이터의 보정 작업 수행
 - Step 4(Integration ETL)
    • (이름, 값, 구조로 인한)데이터 충돌을 해소하고, 클렌징된 데이터를 통합
 - Step 5(Denormalizing ETL)
    • 운영 보고서 생성
    • 혹은 데이터 웨어하우스 또는 데이터 마트에 대한 데이터 적재를 위해 데이터 비정규화 수행

### 
> ETL 이라는 작업은 업무 유형에 따라 다양한 프로세스를 가질 수 있다. 하지만 일반적으로는 분석을 여러 곳에 분산 되어 있는 운영 데이터를 분석용 데이터 저장소로
> 분석 프로세스에 맞는 형태로 변환 하여 저장하는 것을 의미 한다. 
> ETL의 단계가 경우에 따라서는 사용되는 엔진과 저장소가 동일한 서비스 (redshift, dw , spark ) 등이 될 수도 있고 각각의 단계별로 다른 엔진을 사용하거나 ( glue, lambda ) 혹은 저장소가 없이
> 자체 스테이징 저장소를 사용해서 진행될 수도 있다. ( kafka, data factory ) 
> ETL을 사용할지 ELT를 사용할지는 다르나 리소스 활용 측면에서 고민이 필요함. staging 저장소를 (메모리든 스토리지든) 어디에 둘 것인가에 대한 문제로 고민해야 함. 
> computing과 storage가 완전히 분리되고 각 작업에 맞게 리소스 할당이 가능한 클라우드의 경우 ETL 작업보다는 ELT 작업을 통해 데이터를 오브젝트 스토리지에 저장한 후 



## data integration ( 데이터 통합 ) 
 >원천데이터가 서로 다른 형태로 다양하게 존재하는 상황에서 데이터를 통합한다는 것은 시스템을 맞추는 것을 넘어 개념적인 데이터 모델로 정립하여야 하고 관련하여 파생된느 다양한 문제를 조화롭게 해결하는 것으로 정의할 수 있다.

먼저 데이터를 한곳에 모은다고 하면 어떤 데이터를 모을 것인지 정의하고 클라우드 서비스를 사용한다고 하면 AWS Redshift 혹은 S3를 상정하고 혹시나 포함될 수 있는 개인정보도 사전에 식별하여 마스킹 등을 통해 익명화시켜야 되며 데이터 혈통(Data Lineage)도 구축하여 투명성과 가시성도 확보한다.


## datawarehouse 

### modeling
